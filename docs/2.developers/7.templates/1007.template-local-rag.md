---
title: "RAG pipeline run locally with up-to-date knowledge: get answers based on documents stored locally"
description: "End-to-end template to launch a RAG pipeline relying on local computations and models."
article:
    tags: ['showcase', 'llm']
    date: '2024-01-01'
    related: false
author: "pathway"
keywords: ['LLM', 'RAG', 'GPT', 'OpenAI', 'vector store', 'indexing', 'HuggingFace', 'sentence transformers', 'local', 'docker']
github_link: "https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/local"
docker_link: "https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/local"
---

:ArticleFromUrl{url="local"}
