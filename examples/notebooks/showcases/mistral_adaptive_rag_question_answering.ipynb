{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-colab"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pathwaycom/pathway/blob/main/examples/notebooks/showcases/mistral_adaptive_rag_question_answering.ipynb\" target=\"_parent\"><img src=\"https://pathway.com/assets/colab-badge.svg\" alt=\"Run In Colab\" class=\"inline\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pathway with Python 3.10+\n",
        "\n",
        "In the cell below, we install Pathway into a Python 3.10+ Linux runtime.\n",
        "\n",
        "> **If you are running in Google Colab, please run the colab notebook (Ctrl+F9)**, disregarding the 'not authored by Google' warning.\n",
        "> \n",
        "> **The installation and loading time is less than 1 minute**.\n"
      ],
      "metadata": {
        "id": "notebook-instructions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-display\n",
        "!pip install --prefer-binary pathway"
      ],
      "metadata": {
        "id": "pip-installation-pathway",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e828b160",
      "metadata": {},
      "source": [
        "# Private RAG with Adaptive Retrieval using Mistral, Ollama and Pathway"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3a2c5d8",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "In our previous [Adaptive RAG](https://pathway.com/developers/showcases/adaptive-rag) showcase, we showed how to improve traditional RAG in terms of accuracy while maintaining cost and time efficiency. We used OpenAI LLM and embedder in that article. This showcase is replicating that work with local embedders and LLMs.\n",
        "\n",
        "In this showcase, we explore how to set up a private RAG pipeline with adaptive retrieval using [Pathway](https://pathway.com/developers/api-docs/pathway), and Ollama. \n",
        "Our pipeline answers questions from the Stanford Question Answering Dataset [(SQUAD)](https://rajpurkar.github.io/SQuAD-explorer/) using a selection of Wikipedia pages from the same dataset, splitted into paragraphs.\n",
        "\n",
        "We use a local Mistral model as the LLM of choice, deployed with Ollama. We chose Mistral 7B for its performance and small weight footprint, allowing it to be more accessible and faster.\n",
        "\n",
        "We set up our vector store using Pathway with an open source embedding model from the HuggingFace.\n",
        "\n",
        "If you are not familiar with the Pathway, refer to [Pathway xpacks docs](https://pathway.com/developers/api-docs/pathway-xpacks-llm/llms).\n",
        "\n",
        "We explore how to use Pathway to;\n",
        "- load and index documents\n",
        "- connect to our local LLM\n",
        "- prompt our LLM with relevant context, and adaptively add more documents as needed\n",
        "- combine everything, and orchastrate the RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91834a3c",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Retrieval Augmented Generation (RAG) allows Large Language Models (LLMs) to answer questions based on knowledge not present in the original training set. At [Pathway](pathway.com) we use RAG to build [document intelligence solutions](/solutions/rag-pipelines) that answer questions based on private document collections, such as a repository of legal contracts. We are constantly working on improving the accuracy and explainability of our models while keeping the costs low. In this blog post, we share a trick that helped us reach those goals.\n",
        "\n",
        "A typical RAG Question Answering procedure works in two steps. First the question is analyzed and a number of relevant documents are retrieved from a database, typically using a similarity search inside a vector space created by a neural embedding model. Second, retrieved documents are pasted, along with the original question, into a prompt which is sent to the LLM. Thus, the LLM answers the question within a relevant context.\n",
        "\n",
        "Practical implementations of the RAG procedure need to specify the number of documents put into the prompt. A large number of documents increases the ability of the LLM to provide a correct answer, but also increases LLM costs, which typically grow linearly with the length of the provided prompt. The prompt size also influences model explainability: retrieved context documents explain and justify the answer of the LLM and the fewer context documents are needed, the easier it is to verify and trust model outputs.\n",
        "\n",
        "Thus the context size, given by the number of considered documents in a RAG setup, must be chosen to balance costs, desired answer quality, and explainability. However, can we do better than using the same context size regardless of the question to be answered? Intuitively, not all questions are equally hard and some can be answered using a small number of supporting documents, while some may require the LLM to consult a larger prompt. We can confirm this by running a question answering experiment.\n",
        "\n",
        "## Adaptive RAG\n",
        "We can use the model\u2019s refusal to answer questions as a form of model introspection which enables an adaptive RAG question answering strategy:\n",
        "\n",
        "::card\n",
        "#title\n",
        "Adaptive Rag Idea\n",
        "#description\n",
        "Ask the LLM with a small number of context documents. If it refuses to answer, repeat the question with a larger prompt.\n",
        "::\n",
        "\n",
        "This RAG scheme adapts to the hardness of the question and the quality of the retrieved supporting documents using the feedback from the LLM - for most documents a single LLM call with a small prompt is sufficient, and there is no need for auxiliary LLM calls to e.g. guess an initial supporting document count for a question. However, a fraction of questions will require re-asking or rere-asking the LLM.\n",
        "\n",
        "To solve it, we can use a geometric series to expand the prompt with retrieved documents.\n",
        "\n",
        "To learn more about how we do it, and the observed benefits, please see the original work [here.](https://pathway.com/developers/showcases/adaptive-rag)\n",
        "\n",
        "## What is (local) private RAG, do I need it?\n",
        "\n",
        "Most of the RAG applications require you to send your documents & data to propriety APIs. This is a concern for most of the organizations as data privacy with sensitive documents becomes an issue. Generally, you need to send your documents during indexing and LLM question answering stages. \n",
        "\n",
        "To tackle this, you can use locally deployed LLMs and embedders in your RAG pipeline. We eliminate the need to go to proprierty APIs with the help of Ollama, HuggingFace and Pathway.\n",
        "Everything is staying local on your machine.\n",
        "\n",
        "### Why use local LLMs?\n",
        "\n",
        "There are several reasons why you may want to use local or open-source LLMs over propriety ones. \n",
        "- Building RAG applications require documents to be sent to the LLM, this may be an issue in some organizations with sensitive data. \n",
        "- We have observed that some LLMs that are served over the API are regressing in terms of accuracy, with local models, this won't be an issue.\n",
        "- It is possible to fine-tune local LLMs to behave in certain ways or achieve domain adaptation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36edcb59",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "## Using Adaptive RAG locally with Pathway\n",
        "\n",
        "In the cell below, we install Pathway into a Python 3.10+ Linux runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a6f3b8d3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:32.480023Z",
          "iopub.status.busy": "2024-04-24T04:36:32.479900Z",
          "iopub.status.idle": "2024-04-24T04:36:38.966088Z",
          "shell.execute_reply": "2024-04-24T04:36:38.965688Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "!pip install -U --prefer-binary pathway\n",
        "!pip install \"litellm>=1.35\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bf83a1d",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "Download `adaptive-rag-contexts.jsonl` with ~1000 contexts from the SQUAD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7724ccc0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:38.967880Z",
          "iopub.status.busy": "2024-04-24T04:36:38.967742Z",
          "iopub.status.idle": "2024-04-24T04:36:39.360652Z",
          "shell.execute_reply": "2024-04-24T04:36:39.359836Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "!wget -q -nc https://public-pathway-releases.s3.eu-central-1.amazonaws.com/data/adaptive-rag-contexts.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "16499510",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:39.362071Z",
          "iopub.status.busy": "2024-04-24T04:36:39.361953Z",
          "iopub.status.idle": "2024-04-24T04:36:41.385947Z",
          "shell.execute_reply": "2024-04-24T04:36:41.385613Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pathway as pw\n",
        "from pathway.stdlib.indexing import VectorDocumentIndex\n",
        "from pathway.xpacks.llm import embedders\n",
        "from pathway.xpacks.llm.llms import LiteLLMChat\n",
        "from pathway.xpacks.llm.question_answering import answer_with_geometric_rag_strategy_from_index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d4f2081",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "For the embeddings, we provide few selected models that can be used to replicate the work\n",
        "In case you have access to limited computation, we also provide snippet on how to use Mistral embeddings from the API below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2e03260e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:41.387563Z",
          "iopub.status.busy": "2024-04-24T04:36:41.387304Z",
          "iopub.status.idle": "2024-04-24T04:36:41.389036Z",
          "shell.execute_reply": "2024-04-24T04:36:41.388823Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# embedder = LiteLLMEmbedder(\n",
        "#     capacity = 5,\n",
        "#     retry_strategy = pw.udfs.ExponentialBackoffRetryStrategy(max_retries=4, initial_delay=1200),\n",
        "#     model = \"mistral/mistral-embed\",\n",
        "#     api_key=<mistral_api_key>,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cafe530",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "Here are few embedding models that have performed well in our tests\n",
        "These models were selected from the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)\n",
        "\n",
        "We use `pathway.xpacks.llm.embedders` module to load open source embedding models from the HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2ba03afa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:41.390056Z",
          "iopub.status.busy": "2024-04-24T04:36:41.389933Z",
          "iopub.status.idle": "2024-04-24T04:36:44.205352Z",
          "shell.execute_reply": "2024-04-24T04:36:44.204942Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# large_model = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
        "# medium_model = \"avsolatorio/GIST-Embedding-v0\"\n",
        "small_model = \"avsolatorio/GIST-small-Embedding-v0\"\n",
        "\n",
        "embedder = embedders.SentenceTransformerEmbedder(small_model, call_kwargs={\"show_progress_bar\": False})  # disable verbose logs\n",
        "embedding_dimension: int = len(embedder.__wrapped__(\".\"))  # call the model once to get the embedding_dim\n",
        "print(\"Embedding dimension:\", embedding_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "94c0b222",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:44.207044Z",
          "iopub.status.busy": "2024-04-24T04:36:44.206582Z",
          "iopub.status.idle": "2024-04-24T04:36:44.213253Z",
          "shell.execute_reply": "2024-04-24T04:36:44.212984Z"
        }
      },
      "outputs": [],
      "source": [
        "# Load documents in which answers will be searched\n",
        "class InputSchema(pw.Schema):\n",
        "    doc: str\n",
        "    \n",
        "documents = pw.io.fs.read(\n",
        "    \"adaptive-rag-contexts.jsonl\",\n",
        "    format=\"json\",\n",
        "    schema=InputSchema,\n",
        "    json_field_paths={\"doc\": \"/context\"},\n",
        "    mode=\"static\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ebf0e474",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:44.214510Z",
          "iopub.status.busy": "2024-04-24T04:36:44.214262Z",
          "iopub.status.idle": "2024-04-24T04:36:44.218920Z",
          "shell.execute_reply": "2024-04-24T04:36:44.218656Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Create table with example questions\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"query\": [\n",
        "            \"When it is burned what does hydrogen make?\",\n",
        "            \"What was undertaken in 2010 to determine where dogs originated from?\",\n",
        "            # \"What did Arnold's journey into politics look like?\",\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "query = pw.debug.table_from_pandas(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "faa62568",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:44.220119Z",
          "iopub.status.busy": "2024-04-24T04:36:44.219898Z",
          "iopub.status.idle": "2024-04-24T04:36:44.221449Z",
          "shell.execute_reply": "2024-04-24T04:36:44.221205Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Uncomment line below, if you want to check if documents are correctly loaded\n",
        "# documents  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "340dc8a7",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "#### Deploying and using a local LLM\n",
        "Due to its popularity and ease of use, we decided to run the `Mistral 7B` on `Ollama`\n",
        "\n",
        "In order to run local LLM, refer to these steps:\n",
        "- Download Ollama from [ollama.com/download](https://ollama.com/download)\n",
        "- In your terminal, run `ollama serve`\n",
        "- In another terminal, run `ollama run mistral`\n",
        "\n",
        "You can now test it with the following:\n",
        "\n",
        "```bash\n",
        "curl -X POST http://localhost:11434/api/generate -d '{\n",
        "  \"model\": \"mistral\",\n",
        "  \"prompt\":\"Here is a story about llamas eating grass\"\n",
        " }'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7360a6aa",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:44.222466Z",
          "iopub.status.busy": "2024-04-24T04:36:44.222282Z",
          "iopub.status.idle": "2024-04-24T04:36:44.600675Z",
          "shell.execute_reply": "2024-04-24T04:36:44.600291Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# we specifically instruct LLM to return json. in this format, LLM follows the instructions more strictly\n",
        "# this is not needed in gpt-3.5-turbo and mistral-large, but necessary in mistral-7b\n",
        "\n",
        "model = LiteLLMChat(\n",
        "    model=\"ollama/mistral\",\n",
        "    temperature=0,\n",
        "    top_p=1,\n",
        "    api_base=\"http://localhost:11434\",  # local deployment\n",
        "    format=\"json\",  # only available in Ollama local deploy, not usable in Mistral API\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de522d81",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "Create the index with documents and embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a191f326",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:44.602346Z",
          "iopub.status.busy": "2024-04-24T04:36:44.602133Z",
          "iopub.status.idle": "2024-04-24T04:36:44.717432Z",
          "shell.execute_reply": "2024-04-24T04:36:44.717134Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "index = VectorDocumentIndex(\n",
        "    documents.doc, documents, embedder, n_dimensions=embedding_dimension\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53c5b90a",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "Create the adaptive rag table with created index, LLM, embedder, documents, and hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fa6b638e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:44.718653Z",
          "iopub.status.busy": "2024-04-24T04:36:44.718551Z",
          "iopub.status.idle": "2024-04-24T04:36:47.891877Z",
          "shell.execute_reply": "2024-04-24T04:36:47.891478Z"
        },
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "result = query.select(\n",
        "    question=query.query,\n",
        "    result=answer_with_geometric_rag_strategy_from_index(\n",
        "        query.query,\n",
        "        index,\n",
        "        documents.doc,\n",
        "        model,\n",
        "        n_starting_documents=2,\n",
        "        factor=2,\n",
        "        max_iterations=4,\n",
        "        strict_prompt=True,  # needed for open source models, instructs LLM to give JSON output strictly\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e97024d",
      "metadata": {},
      "source": [
        "Run and store the results in Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f5889fbf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-04-24T04:36:47.893508Z",
          "iopub.status.busy": "2024-04-24T04:36:47.893403Z",
          "iopub.status.idle": "2024-04-24T04:36:47.895041Z",
          "shell.execute_reply": "2024-04-24T04:36:47.894789Z"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment this to run the notebook\n",
        "# responses_df = pw.debug.table_to_pandas(result)\n",
        "\n",
        "# print(responses_df[\"result\"].iloc[0])\n",
        "#\n",
        "# print(responses_df[\"result\"].iloc[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22659d97",
      "metadata": {},
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b20700e4",
      "metadata": {},
      "source": [
        "In the prior showcase, we have shown that adaptive RAG provides a good balance between cost-efficiency and accuracy, and makes improvements over the naive RAG.\n",
        "In this showcase, we demonstrate how to replicate the [Adaptive RAG](https://pathway.com/developers/showcases/adaptive-rag) strategy with a fully local setup, including local embedder and LLM. To achieve this, we had to make some modifications on the original prompts, and how the LLM is called. \n",
        "\n",
        "To ensure stricter adherence to instructions, especially with smaller open-source LLMs, we employed the `json` mode for LLM outputs. This approach enhances control over the model's response, leading to more predictable behavior.\n",
        "\n",
        "\n",
        "Notably, this setup can be run entirely locally with open-source LLMs, making it ideal for organizations with sensitive data or who have already deployed local LLMs."
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}