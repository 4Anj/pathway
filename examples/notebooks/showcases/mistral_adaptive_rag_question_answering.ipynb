{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-colab"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pathwaycom/pathway/blob/main/examples/notebooks/showcases/mistral_adaptive_rag_question_answering.ipynb\" target=\"_parent\"><img src=\"https://pathway.com/assets/colab-badge.svg\" alt=\"Run In Colab\" class=\"inline\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Pathway with Python 3.10+\n",
        "\n",
        "In the cell below, we install Pathway into a Python 3.10+ Linux runtime.\n",
        "\n",
        "> **If you are running in Google Colab, please run the colab notebook (Ctrl+F9)**, disregarding the 'not authored by Google' warning.\n",
        "> \n",
        "> **The installation and loading time is less than 1 minute**.\n"
      ],
      "metadata": {
        "id": "notebook-instructions"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture --no-display\n",
        "!pip install --prefer-binary pathway"
      ],
      "metadata": {
        "id": "pip-installation-pathway",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1",
      "metadata": {},
      "source": [
        "# Private RAG with Adaptive Retrieval using Mistral, Ollama and Pathway"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "Retrieval Augmented Generation (RAG) is a powerful way to answer questions based on your own, private, knowledge database.\n",
        "However, data security is key: sensitive information like R&D secrets, GDPR-protected data, and internal documents cannot be entrusted to third parties.\n",
        "\n",
        "Most of the existing RAG solutions rely on LLM APIs that send your data, at least a part of it, to the LLM provider.\n",
        "For example, if your RAG solution uses ChatGPT, you will have to send your data to OpenAI through OpenAI API.\n",
        "\n",
        "Fortunately, there is a solution to keep your data private: deploying a local LLM.\n",
        "By deploying everything locally, your data remains secure within own infrastructure.\n",
        "It eliminates the risk of sensitive information ever leaving your control.\n",
        "\n",
        "While this seems quite an engineering feat, don't worry Pathway provides you everything you need to make this as easy as possible.\n",
        "\n",
        "In this showcase, you will learn how to set up a private RAG pipeline with adaptive retrieval using Pathway, Mistral, and Ollama.\n",
        "The pipeline answers questions from the Stanford Question Answering Dataset [(SQUAD)](https://rajpurkar.github.io/SQuAD-explorer/) using a selection of Wikipedia pages from the same dataset as the context, split into paragraphs.\n",
        "**This RAG pipeline runs without any API access or any data leaving the local machine with Pathway**.\n",
        "\n",
        "![Reference architecture](https://pathway.com/assets/content/blog/local-adaptive-rag/local_adaptive.png)\n",
        "\n",
        "Our RAG app will use a Mistral 7B, locally deployed using Ollama. Mistral 7B is chosen for its performance and efficient size.\n",
        "The pipeline uses Pathway vector store & live document indexing pipeline with an open-source embedding model from the HuggingFace.\n",
        "\n",
        "Pathway brings support for real-time data synchronization pipelines out of the box, and possibility of secure private document handling with enterprise connectors for synchronizing Sharepoint and Google Drive incrementally.\n",
        "Here, Pathway's built-in vector index is used.\n",
        "\n",
        "If you are not familiar with the Pathway, refer to [overview of Pathway's LLM xpack](https://pathway.com/developers/user-guide/llm-xpack/overview) and [its documentation](https://pathway.com/developers/api-docs/pathway-xpacks-llm/llms).\n",
        "This article is an extension of our previous [Adaptive RAG showcase](/developers/showcases/adaptive-rag) on how to improve RAG accuracy while maintaining cost and time efficiency.\n",
        "The main difference is that showcase was using OpenAI LLM and `text-embedding-ada-002 embedder` while in the following the pipeline will use locally-hosted LLMs and embedders.\n",
        "\n",
        "You will explore how to use Pathway to:\n",
        "- load and index documents\n",
        "- connect to our local LLM\n",
        "- prompt our LLM with relevant context, and adaptively add more documents as needed\n",
        "- combine everything, and orchestrate the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "## What is Private RAG and Why Do You Need It?\n",
        "\n",
        "Most of the RAG applications require you to send your documents & data to propriety APIs. This is a concern for most organizations as data privacy with sensitive documents becomes an issue. Generally, you need to send your documents during the indexing and LLM question-answering stages.\n",
        "\n",
        "To tackle this, you can use a **private RAG: locally deployed LLMs and embedders in your RAG pipeline**.\n",
        "You don't need to go to proprietary APIs with the help of Ollama, HuggingFace and Pathway.\n",
        "Everything is staying local on your machine.\n",
        "\n",
        "### Why use Local LLMs?\n",
        "\n",
        "There are several advantages to using local LLMs (Large Language Models) over cloud-based APIs:\n",
        "- **Data Privacy**: Local LLMs keep your sensitive data on your machines.\n",
        "- **Accuracy**: Cloud-based LLM accuracy can sometimes regress, whereas you can fine-tune local models for better performance.\n",
        "- **Customization**: Local LLMs allow for fine-tuning to achieve specific behaviors or domain adaptation.\n",
        "\n",
        "### Mistral and Ollama for privacy\n",
        "\n",
        "**Mistral 7B** is publicly available LLM model release by [Mistral AI](https://mistral.ai/).\n",
        "Its (relative) small size of 7.3B parameters and impressive performances make Mistral 7B a perfect candidate for a local deployment.\n",
        "\n",
        "The pipeline relies on **Ollama** to deploy the Mistral 7B model.\n",
        "[Ollama](https://ollama.com/) is a tool to create, manage, and run LLM models.\n",
        "Ollama is used to download and configure locally the Mistral 7B model.\n",
        "\n",
        "## Pathway Adaptive RAG\n",
        "\n",
        "Standard RAG process first retrieves a fixed number of documents to answer the question and then build a personalized prompt using the documents to allow the LLM to answer the question with a relevant context.\n",
        "The number of retrieved document is a tradeoff: a large number of documents increases the ability of the LLM to provide a correct answer but also increases LLM costs.\n",
        "\n",
        "Pathway Adaptive RAG improves this tradeoff by adapting the number of retrieved documents depending on the difficulty of the question.\n",
        "First, a small number of context documents is retrieved and if the LLM refuses to answer, repeat the question with a larger prompt.\n",
        "This process is repeated until the LLM got a answer.\n",
        "\n",
        "To learn more about how we do it, and the observed benefits, please see the original work [here](/developers/showcases/adaptive-rag)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "## Private RAG with Pathway Adaptive RAG\n",
        "\n",
        "This section provides a step-by-step guide on how to set up Private RAG with Adaptive Retrieval using Pathway, a framework for building LLM applications. The guide covers:\n",
        "1. **Installation**: Installing Pathway and required libraries.\n",
        "2. **Data Loading**: Loading documents for which answer retrieval will be performed.\n",
        "3. **Embedding Model Selection**: Choosing an open-source embedding model from Hugging Face.\n",
        "4. **Local LLM Deployment**: Instructions on deploying a local LLM using Ollama, a lightweight container runtime.\n",
        "5. **LLM Initialization**: Setting up the LLM instance to interact with the local model.\n",
        "6. **Vector Document Index Creation**: Building an index for efficient document retrieval using the embedding model.\n",
        "7. **Adaptive RAG Table Creation**: Defining the parameters for the Adaptive RAG strategy.\n",
        "8. **Pipeline Execution**: Running the Private RAG pipeline with your data.\n",
        "\n",
        "### 1. Installation\n",
        "\n",
        "You install Pathway into a Python 3.10+ Linux runtime with a simple pip command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "%%capture --no-display\n",
        "!pip install -U --prefer-binary pathway\n",
        "!pip install \"litellm>=1.35\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "### 2. Data Loading\n",
        "Download `adaptive-rag-contexts.jsonl` with ~1000 contexts from the SQUAD dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "!wget -q -nc https://public-pathway-releases.s3.eu-central-1.amazonaws.com/data/adaptive-rag-contexts.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pathway as pw\n",
        "from pathway.stdlib.indexing import VectorDocumentIndex\n",
        "from pathway.xpacks.llm import embedders\n",
        "from pathway.xpacks.llm.llms import LiteLLMChat\n",
        "from pathway.xpacks.llm.question_answering import (\n",
        "    answer_with_geometric_rag_strategy_from_index,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "### 3. Embedding Model Selection\n",
        "\n",
        "For the embeddings, we provide a few selected models that can be used to replicate the work.\n",
        "In case you have access to limited computation, and want to use an embedder over the API, we also provide a snippet on how to use Mistral embeddings below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# embedder = LiteLLMEmbedder(\n",
        "#     capacity = 5,\n",
        "#     retry_strategy = pw.udfs.ExponentialBackoffRetryStrategy(max_retries=4, initial_delay=1200),\n",
        "#     model = \"mistral/mistral-embed\",\n",
        "#     api_key=<mistral_api_key>,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "Here are a few embedding models that have performed well in our tests\n",
        "These models were selected from the [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard).\n",
        "\n",
        "We use `pathway.xpacks.llm.embedders` module to load open-source embedding models from the HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# large_model = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
        "# medium_model = \"avsolatorio/GIST-Embedding-v0\"\n",
        "small_model = \"avsolatorio/GIST-small-Embedding-v0\"\n",
        "\n",
        "embedder = embedders.SentenceTransformerEmbedder(\n",
        "    small_model, call_kwargs={\"show_progress_bar\": False}\n",
        ")  # disable verbose logs\n",
        "embedding_dimension: int = len(\n",
        "    embedder.__wrapped__(\".\")\n",
        ")  # call the model once to get the embedding_dim\n",
        "print(\"Embedding dimension:\", embedding_dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load documents in which answers will be searched\n",
        "class InputSchema(pw.Schema):\n",
        "    doc: str\n",
        "\n",
        "\n",
        "documents = pw.io.fs.read(\n",
        "    \"adaptive-rag-contexts.jsonl\",\n",
        "    format=\"json\",\n",
        "    schema=InputSchema,\n",
        "    json_field_paths={\"doc\": \"/context\"},\n",
        "    mode=\"static\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if documents are correctly loaded\n",
        "# documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# Create a table with example questions\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        \"query\": [\n",
        "            \"When it is burned what does hydrogen make?\",\n",
        "            \"What was undertaken in 2010 to determine where dogs originated from?\",\n",
        "            # \"What did Arnold's journey into politics look like?\",\n",
        "        ]\n",
        "    }\n",
        ")\n",
        "query = pw.debug.table_from_pandas(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "#### 4. Local LLM Deployement\n",
        "Due to its size, performances, and ease of use, we decided to run the `Mistral 7B` on `Ollama`\n",
        "\n",
        "In order to run local LLM, refer to these steps:\n",
        "- Download Ollama from [ollama.com/download](https://ollama.com/download)\n",
        "- In your terminal, run `ollama serve`\n",
        "- In another terminal, run `ollama run mistral`\n",
        "\n",
        "You can now test it with the following:\n",
        "\n",
        "```bash\n",
        "curl -X POST http://localhost:11434/api/generate -d '{\n",
        "  \"model\": \"mistral\",\n",
        "  \"prompt\":\"Here is a story about llamas eating grass\"\n",
        " }'\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17",
      "metadata": {},
      "source": [
        "### 5. LLM Initialization\n",
        "\n",
        "Initialize the LLM instance that will call our local model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "# we specifically instruct LLM to return json. in this format, LLM follows the instructions more strictly\n",
        "# this is not needed in gpt-3.5-turbo and mistral-large, but necessary in mistral-7b\n",
        "\n",
        "model = LiteLLMChat(\n",
        "    model=\"ollama/mistral\",\n",
        "    temperature=0,\n",
        "    top_p=1,\n",
        "    api_base=\"http://localhost:11434\",  # local deployment\n",
        "    format=\"json\",  # only available in Ollama local deploy, not usable in Mistral API\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "### 6. Vector Document Index Creation\n",
        "Create the index with documents and embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "index = VectorDocumentIndex(\n",
        "    documents.doc, documents, embedder, n_dimensions=embedding_dimension\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "### 7. Adaptive RAG Table Creation\n",
        "Create the adaptive rag table with created index, LLM, embedder, documents, and hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "result = query.select(\n",
        "    question=query.query,\n",
        "    result=answer_with_geometric_rag_strategy_from_index(\n",
        "        query.query,\n",
        "        index,\n",
        "        documents.doc,\n",
        "        model,\n",
        "        n_starting_documents=2,\n",
        "        factor=2,\n",
        "        max_iterations=4,\n",
        "        strict_prompt=True,  # needed for open source models, instructs LLM to give JSON output strictly\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "source": [
        "### 8. Pipeline Execution\n",
        "You can run the pipeline and print the results table with `pw.debug.compute_and_print`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24",
      "metadata": {
        "lines_to_next_cell": 2
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hydrogen makes water when burned [2].\n",
            "\n",
            "Extensive genetic studies were conducted during the 2010s which indicated that dogs diverged from an extinct wolf-like canid in Eurasia around 40,000 years ago.\n"
          ]
        }
      ],
      "source": [
        "pw.debug.compute_and_print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25",
      "metadata": {},
      "source": [
        "# Going to Production"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26",
      "metadata": {},
      "source": [
        "Now you have an fully private adaptive RAG. Not only the adaptive retrieval improve the accuracy of the RAG, but by using a local LLM model, all your data remain safe on your system.\n",
        "You can go further by building and deploying your RAG application in production with Pathway, including serving the endpoints. \n",
        "\n",
        "To learn more, check out our [question answering demo](https://github.com/pathwaycom/llm-app/tree/main/examples/pipelines/demo-question-answering) for brief introduction on how to use Pathway to build a full RAG application."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27",
      "metadata": {},
      "source": [
        "# Key Takeaways"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28",
      "metadata": {},
      "source": [
        "While [Adaptive RAG](https://pathway.com/developers/showcases/adaptive-rag) provides a good balance between cost-efficiency and accuracy, and makes improvements over the naive RAG, it was relying -as most of the RAG solutions- on third parties API.\n",
        "In this showcase, you have learned how to do design a **fully local and private adaptive RAG** setup, including local embedder and LLM.\n",
        "Using Pathway, only small modifications to the original prompts and parameters were needed to provide privacy to the RAG pipeline.\n",
        "\n",
        "To ensure stricter adherence to instructions, especially with smaller open-source LLMs, we employed the `json` mode for LLM outputs. This approach enhances control over the model's response, leading to more predictable behavior.\n",
        "\n",
        "**This private RAG setup can be run entirely locally with open-source LLMs, making it ideal for organizations with sensitive data or who have already deployed local LLMs.**"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}